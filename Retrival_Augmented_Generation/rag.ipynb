{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://medium.com/@sahin.samia/what-is-retrieval-augmented-generation-rag-in-llm-and-how-it-works-a8c79e35a172\",))\n",
    "\n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "docs=text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of docs 15\n"
     ]
    }
   ],
   "source": [
    "print(\"length of docs\",len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@sahin.samia/what-is-retrieval-augmented-generation-rag-in-llm-and-how-it-works-a8c79e35a172', 'title': 'What is Retrieval-Augmented Generation(RAG) in LLM and How it works? | by Sahin Ahmed, Data Scientist | Medium', 'description': 'In the fast-paced world of artificial intelligence, language models have undergone a remarkable evolution. From the early days of simple rule-based systems to the sophisticated neural networks we see…', 'language': 'en'}, page_content='What is Retrieval-Augmented Generation(RAG) in LLM and How it works? | by Sahin Ahmed, Data Scientist | MediumOpen in appSign upSign inWriteSign upSign inMastodonWhat is Retrieval-Augmented Generation(RAG) in LLM and How it works?Sahin Ahmed, Data Scientist·Follow7 min read·Apr 22, 2024--ListenShareIn the fast-paced world of artificial intelligence, language models have undergone a remarkable evolution. From the early days of simple rule-based systems to the sophisticated neural networks we see today, each step has significantly expanded what AI can do with language. A pivotal development in this journey is the introduction of Retrieval-Augmented Generation, or RAG.RAG represents a blend of traditional language models with an innovative twist: it integrates information retrieval directly into the generation process. Think of it as having an AI that can look up information in a library of texts before responding, making it more knowledgeable and context-aware. This capability is not')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store=Chroma.from_documents(documents=docs,embedding=OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectore_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived_docs=retriever.invoke(\"What is RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval.Potential Future Enhancements:Personalization: Integrating user profiles and historical interactions to personalize responses, making RAG models more effective in customer service and recommendation systems.Knowledge Grounding: Using external knowledge bases not just for retrieval but also for grounding the responses in verifiable facts, which is crucial for educational and informational applications.Efficient Indexing: Employing more efficient data structures and algorithms for indexing the database to speed up retrieval and reduce computational costs.Artificial IntelligenceAIMachine LearningLarge Language ModelsData Science----FollowWritten by Sahin Ahmed, Data Scientist138 FollowersData Scientist | MSc Data science|Lifelong Learner | Making an Impact through Data Science | Machine Learning| Deep Learning |NLP| Statistical ModelingFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\n"
     ]
    }
   ],
   "source": [
    "print(retrived_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A2BC6BF730>, search_kwargs={})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver=vectore_store.as_retriever()\n",
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Retrieval-Augmented Generation (RAG) is a technique used in language models to improve their accuracy and knowledge by incorporating external data during the generation process. RAG works by retrieving relevant information from a large dataset or knowledge base before passing it along to the Large Language Model (LLM) to generate a response. The LLM uses both the original query and the retrieved contexts to produce a comprehensive and accurate response, ensuring that it is not just plausible but also rich in detail and accuracy.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is RAG?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
