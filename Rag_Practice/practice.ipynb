{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Using cached pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Using cached pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "def extract_with_pdf(path):\n",
    "    text =\"\"\n",
    "    with pdfplumber.open(path) as pdfreader:\n",
    "        for page in pdfreader.pages:\n",
    "            text+=page.extract_text() or \"\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extracted=extract_with_pdf(r\"D:\\waste for git\\Generative_AI\\Rag\\attention all you need.pdf\")\n",
    "#print(text_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Langchain in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (3.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (0.3.8)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (0.1.131)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from Langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.13.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->Langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->Langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.8->Langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->Langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->Langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->Langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->Langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->Langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->Langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->Langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->Langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->Langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from SQLAlchemy<3,>=1.4->Langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->Langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->Langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->Langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->Langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->Langchain) (3.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->Langchain) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Attention Is All You Need\\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\\navaswani@google.com noam@google.com nikip@google.com usz@google.com\\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\\nGoogleResearch UniversityofToronto GoogleBrain\\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\\nIlliaPolosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor', 'convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly', 'less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.0after\\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\\nbestmodelsfromtheliterature.\\n1 Introduction\\nRecurrentneuralnetworks,longshort-termmemory[12]andgatedrecurrent[7]neuralnetworks', 'inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[29,2,5]. Numerous\\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\\narchitectures[31,21,13].\\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand', 'hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery\\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand', 'implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\\nourresearch.\\n†WorkperformedwhileatGoogleBrain.\\n‡WorkperformedwhileatGoogleResearch.\\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden', 'statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently\\nt t−1\\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[18]andconditional\\ncomputation[26],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\\nconstraintofsequentialcomputation,however,remains.', 'Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\\ntheinputoroutputsequences[2,16]. Inallbutafewcases[22],however,suchattentionmechanisms\\nareusedinconjunctionwitharecurrentnetwork.\\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.', 'TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin\\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\\n2 Background\\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\\n[20],ByteNet[15]andConvS2S[8],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,', 'thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\\nit more difficult to learn dependencies between distant positions [11]. In the Transformer this is\\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribedinsection3.2.', 'describedinsection3.2.\\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\\nofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\\ntextualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19].\\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-', 'alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\\nlanguagemodelingtasks[28].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\\nself-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8].\\n3 ModelArchitecture', '3 ModelArchitecture\\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29].\\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\\n1 n\\nof continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\\n1 n\\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\\n1 m\\n[9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.', 'TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\\nrespectively.\\n3.1 EncoderandDecoderStacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\\n2Figure1: TheTransformer-modelarchitecture.', '2Figure1: TheTransformer-modelarchitecture.\\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[10]aroundeachof\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\\nlayers,produceoutputsofdimensiond =512.\\nmodel', 'layers,produceoutputsofdimensiond =512.\\nmodel\\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This', 'masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\\n3.2 Attention\\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\\nquerywiththecorrespondingkey.\\n3.2.1 ScaledDot-ProductAttention', '3.2.1 ScaledDot-ProductAttention\\nWecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2). Theinputconsistsof\\nqueriesandkeysofdimensiond ,andvaluesofdimensiond . Wecomputethedotproductsofthe\\nk v\\n3ScaledDot-ProductAttention Multi-HeadAttention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattentionlayersrunninginparallel.\\n√\\nquerywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe\\nk\\nvalues.', 'k\\nvalues.\\nInpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether\\nintoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute\\nthematrixofoutputsas:\\nQKT\\nAttention(Q,K,V)=softmax( √ )V (1)\\nd\\nk\\nThetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi-\\nplicative)attention. Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor', 'of √1 . Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith\\ndk\\nasinglehiddenlayer. Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis\\nmuchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized\\nmatrixmultiplicationcode.\\nWhileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms\\nk\\ndotproductattentionwithoutscalingforlargervaluesofd [3]. Wesuspectthatforlargevaluesof\\nk', 'k\\nd ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas\\nk\\nextremelysmallgradients4. Tocounteractthiseffect,wescalethedotproductsby √1 .\\ndk\\n3.2.2 Multi-HeadAttention\\nInsteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries,\\nmodel\\nwefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned\\nlinearprojectionstod ,d andd dimensions,respectively. Oneachoftheseprojectedversionsof\\nk k v', 'k k v\\nqueries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional\\nv\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepictedinFigure2.\\nMulti-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation\\nsubspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis.\\n4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom', 'variableswithmean0andvariance1.Thentheirdotproduct,q·k=(cid:80)dk\\nq k ,hasmean0andvarianced .\\ni=1 i i k\\n4MultiHead(Q,K,V)=Concat(head ,...,head )WO\\n1 h\\nwherehead =Attention(QWQ,KWK,VWV)\\ni i i i\\nWheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv\\ni i i\\nandWO ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\nd =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost\\nk v model', 'k v model\\nissimilartothatofsingle-headattentionwithfulldimensionality.\\n3.2.3 ApplicationsofAttentioninourModel\\nTheTransformerusesmulti-headattentioninthreedifferentways:\\n• In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\\nandthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\\npositioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31,2,8].', '[31,2,8].\\n• Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\\nandqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\\nencoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\\nencoder.\\n• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto\\nallpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward', 'informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis\\ninsideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput\\nofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.\\n3.3 Position-wiseFeed-ForwardNetworks\\nInadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully\\nconnectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This\\nconsistsoftwolineartransformationswithaReLUactivationinbetween.', 'FFN(x)=max(0,xW +b )W +b (2)\\n1 1 2 2\\nWhilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is d = 512, and the inner-layer has dimensionality\\nmodel\\nd =2048.\\nff\\n3.4 EmbeddingsandSoftmax\\nSimilarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput', 'tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-\\nmodel\\nmationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In\\nourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax\\nlineartransformation,similarto[24]. Intheembeddinglayers,wemultiplythoseweightsby d .\\nmodel\\n3.5 PositionalEncoding\\nSinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe', 'orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe\\ntokensinthesequence. Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe\\n5Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations\\nfordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel\\nsizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.\\nLayerType ComplexityperLayer Sequential MaximumPathLength\\nOperations', 'Operations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(log (n))\\nk\\nSelf-Attention(restricted) O(r·n·d) O(1) O(n/r)\\nbottomsoftheencoderanddecoderstacks. Thepositionalencodingshavethesamedimensiond\\nmodel\\nastheembeddings,sothatthetwocanbesummed. Therearemanychoicesofpositionalencodings,\\nlearnedandfixed[8].\\nInthiswork,weusesineandcosinefunctionsofdifferentfrequencies:\\nPE =sin(pos/100002i/dmodel)\\n(pos,2i)\\nPE =cos(pos/100002i/dmodel)\\n(pos,2i+1)', '(pos,2i)\\nPE =cos(pos/100002i/dmodel)\\n(pos,2i+1)\\nwhereposisthepositionandiisthedimension. Thatis,eachdimensionofthepositionalencoding\\ncorrespondstoasinusoid. Thewavelengthsformageometricprogressionfrom2πto10000·2π. We\\nchosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby\\nrelativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof\\npos+k\\nPE .\\npos\\nWealsoexperimentedwithusinglearnedpositionalembeddings[8]instead,andfoundthatthetwo', 'versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion\\nbecauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered\\nduringtraining.\\n4 WhySelf-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations\\n(x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden\\n1 n 1 n i i', '1 n 1 n i i\\nlayerinatypicalsequencetransductionencoderordecoder. Motivatingouruseofself-attentionwe\\nconsiderthreedesiderata.\\nOneisthetotalcomputationalcomplexityperlayer. Anotheristheamountofcomputationthatcan\\nbeparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.\\nThethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork. Learninglong-range\\ndependenciesisakeychallengeinmanysequencetransductiontasks. Onekeyfactoraffectingthe', 'abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto\\ntraverseinthenetwork. Theshorterthesepathsbetweenanycombinationofpositionsintheinput\\nandoutputsequences,theeasieritistolearnlong-rangedependencies[11]. Hencewealsocompare\\nthemaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe\\ndifferentlayertypes.\\nAsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially', 'executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece\\n[31]andbyte-pair[25]representations. Toimprovecomputationalperformancefortasksinvolving', 'verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin\\n6theinputsequencecenteredaroundtherespectiveoutputposition. Thiswouldincreasethemaximum\\npathlengthtoO(n/r). Weplantoinvestigatethisapproachfurtherinfuturework.\\nAsingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput\\npositions. DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels,', 'orO(log (n))inthecaseofdilatedconvolutions[15], increasingthelengthofthelongestpaths\\nk\\nbetweenanytwopositionsinthenetwork. Convolutionallayersaregenerallymoreexpensivethan\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, toO(k·n·d+n·d2). Evenwithk = n, however, thecomplexityofaseparable\\nconvolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer,\\ntheapproachwetakeinourmodel.', 'theapproachwetakeinourmodel.\\nAssidebenefit,self-attentioncouldyieldmoreinterpretablemodels.Weinspectattentiondistributions\\nfromourmodelsandpresentanddiscussexamplesintheappendix. Notonlydoindividualattention\\nheadsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic\\nandsemanticstructureofthesentences.\\n5 Training\\nThissectiondescribesthetrainingregimeforourmodels.\\n5.1 TrainingDataandBatching', '5.1 TrainingDataandBatching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentencepairs. Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource-\\ntargetvocabularyofabout37000tokens. ForEnglish-French,weusedthesignificantlylargerWMT\\n2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece\\nvocabulary[31].Sentencepairswerebatchedtogetherbyapproximatesequencelength.Eachtraining', 'batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000\\ntargettokens.\\n5.2 HardwareandSchedule\\nWetrainedourmodelsononemachinewith8NVIDIAP100GPUs. Forourbasemodelsusing\\nthehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds. We\\ntrainedthebasemodelsforatotalof100,000stepsor12hours. Forourbigmodels,(describedonthe\\nbottomlineoftable3),steptimewas1.0seconds. Thebigmodelsweretrainedfor300,000steps\\n(3.5days).\\n5.3 Optimizer', '(3.5days).\\n5.3 Optimizer\\nWeusedtheAdamoptimizer[17]withβ =0.9,β =0.98and(cid:15)=10−9. Wevariedthelearning\\n1 2\\nrateoverthecourseoftraining,accordingtotheformula:\\nlrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nmodel\\nThiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps,\\nanddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber. Weused\\nwarmup_steps=4000.\\n5.4 Regularization\\nWeemploythreetypesofregularizationduringtraining:', 'Weemploythreetypesofregularizationduringtraining:\\nResidualDropout Weapplydropout[27]totheoutputofeachsub-layer,beforeitisaddedtothe\\nsub-layerinputandnormalized. Inaddition,weapplydropouttothesumsoftheembeddingsandthe\\npositionalencodingsinboththeencoderanddecoderstacks. Forthebasemodel,weusearateof\\nP =0.1.\\ndrop\\n7Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe\\nEnglish-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.', 'BLEU TrainingCost(FLOPs)\\nModel\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet[15] 23.75\\nDeep-Att+PosUnk[32] 39.2 1.0·1020\\nGNMT+RL[31] 24.6 39.92 2.3·1019 1.4·1020\\nConvS2S[8] 25.16 40.46 9.6·1018 1.5·1020\\nMoE[26] 26.03 40.56 2.0·1019 1.2·1020\\nDeep-Att+PosUnkEnsemble[32] 40.4 8.0·1020\\nGNMT+RLEnsemble[31] 26.30 41.16 1.8·1020 1.1·1021\\nConvS2SEnsemble[8] 26.36 41.29 7.7·1019 1.2·1021\\nTransformer(basemodel) 27.3 38.1 3.3·1018\\nTransformer(big) 28.4 41.0 2.3·1019', 'Transformer(big) 28.4 41.0 2.3·1019\\nLabelSmoothing Duringtraining,weemployedlabelsmoothingofvalue(cid:15) = 0.1[30]. This\\nls\\nhurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.\\n6 Results\\n6.1 MachineTranslation\\nOntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big)\\ninTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0\\nBLEU,establishinganewstate-of-the-artBLEUscoreof28.4. Theconfigurationofthismodelis', 'listedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs. Evenourbasemodel\\nsurpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof\\nthecompetitivemodels.\\nOntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0,\\noutperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe\\npreviousstate-of-the-artmodel. TheTransformer(big)modeltrainedforEnglish-to-Frenchused\\ndropoutrateP =0.1,insteadof0.3.\\ndrop', 'dropoutrateP =0.1,insteadof0.3.\\ndrop\\nForthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which\\nwerewrittenat10-minuteintervals. Forthebigmodels,weaveragedthelast20checkpoints. We\\nusedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[31]. Thesehyperparameters\\nwerechosenafterexperimentationonthedevelopmentset. Wesetthemaximumoutputlengthduring\\ninferencetoinputlength+50,butterminateearlywhenpossible[31].', 'Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel\\narchitecturesfromtheliterature. Weestimatethenumberoffloatingpointoperationsusedtotraina\\nmodelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained\\nsingle-precisionfloating-pointcapacityofeachGPU5.\\n6.2 ModelVariations\\nToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel\\nindifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe', 'developmentset,newstest2013. Weusedbeamsearchasdescribedintheprevioussection,butno\\ncheckpointaveraging. WepresenttheseresultsinTable3.\\nInTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.\\n5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.', '8Table3: VariationsontheTransformerarchitecture. Unlistedvaluesareidenticaltothoseofthebase\\nmodel. AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013. Listed\\nperplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto\\nper-wordperplexities.\\ntrain PPL BLEU params\\nN d d h d d P (cid:15)\\nmodel ff k v drop ls steps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n(A)\\n16 32 32 4.91 25.8', '4 128 128 5.00 25.5\\n(A)\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n16 5.16 25.1 58\\n(B)\\n32 5.01 25.4 60\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n(C) 256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n(D)\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positionalembeddinginsteadofsinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nInTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\\nk', 'k\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunctionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\\nbiggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\\nsinusoidalpositionalencodingwithlearnedpositionalembeddings[8],andobservenearlyidentical\\nresultstothebasemodel.\\n7 Conclusion\\nInthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon', 'attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\\nmulti-headedself-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-Frenchtranslationtasks,weachieveanewstateoftheart. Intheformertaskourbest\\nmodeloutperformsevenallpreviouslyreportedensembles.', 'Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks. We\\nplantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand\\ntoinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs\\nsuchasimages,audioandvideo. Makinggenerationlesssequentialisanotherresearchgoalsofours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.', 'tensorflow/tensor2tensor.\\nAcknowledgements Wearegratefulto NalKalchbrennerand StephanGouwsfor theirfruitful\\ncomments,correctionsandinspiration.\\n9References\\n[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint\\narXiv:1607.06450,2016.\\n[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly\\nlearningtoalignandtranslate. CoRR,abs/1409.0473,2014.\\n[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural', 'machinetranslationarchitectures. CoRR,abs/1703.03906,2017.\\n[4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine\\nreading. arXivpreprintarXiv:1601.06733,2016.\\n[5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk,\\nandYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical\\nmachinetranslation. CoRR,abs/1406.1078,2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv', 'preprintarXiv:1610.02357,2016.\\n[7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation\\nofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.\\n[8] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu-\\ntionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850,2013.', 'arXiv:1308.0850,2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition,pages770–778,2016.\\n[11] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin\\nrecurrentnets: thedifficultyoflearninglong-termdependencies,2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780,1997.', '9(8):1735–1780,1997.\\n[13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring\\nthelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.\\n[14] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference\\nonLearningRepresentations(ICLR),2016.\\n[15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo-\\nrayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2,\\n2017.', '2017.\\n[16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks.\\nInInternationalConferenceonLearningRepresentations,2017.\\n[17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.\\n[18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint\\narXiv:1703.10722,2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen', 'Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130,2017.\\n[20] SamyBengioŁukaszKaiser. Canactivememoryreplaceattention? InAdvancesinNeural\\nInformationProcessingSystems,(NIPS),2016.\\n10[21] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention-\\nbasedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.\\n[22] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention', 'model. InEmpiricalMethodsinNaturalLanguageProcessing,2016.\\n[23] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive\\nsummarization. arXivpreprintarXiv:1705.04304,2017.\\n[24] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv\\npreprintarXiv:1608.05859,2016.\\n[25] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords\\nwithsubwordunits. arXivpreprintarXiv:1508.07909,2015.', '[26] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,\\nandJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts\\nlayer. arXivpreprintarXiv:1701.06538,2017.\\n[27] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi-\\nnov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine\\nLearningResearch,15(1):1929–1958,2014.', 'LearningResearch,15(1):1929–1958,2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors,\\nAdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,\\nInc.,2015.\\n[29] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural\\nnetworks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.', '[30] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.\\nRethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine\\ntranslationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint\\narXiv:1609.08144,2016.', 'arXiv:1609.08144,2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.\\n11']\n",
      "29243\n"
     ]
    }
   ],
   "source": [
    "# dividing into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,         # Maximum size of each chunk\n",
    "        chunk_overlap=50,       # Overlap between chunks\n",
    "        length_function=len,    # Function to calculate length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Possible split points\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "text_chunks = get_text_chunks(text_extracted)\n",
    "print(text_chunks)\n",
    "print(len(text_extracted))\n",
    "\n",
    "#get_text_chunks(text_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpuNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading faiss_cpu-1.9.0-cp310-cp310-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mohammed riyaz\\anaconda3\\envs\\rag\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.9.0-cp310-cp310-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 11.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.2/14.9 MB 11.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.6/14.9 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.9/14.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.3/14.9 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.6/14.9 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.9 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 9.3 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings and storing\n",
    "def get_vector_store(text_chunks):\n",
    "    embeddings=HuggingFaceBgeEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vector_store=FAISS.from_texts(text_chunks,embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "    \n",
    "get_vector_store(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def create_qa_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    Use the following pieces of context to answer the question. If you cannot find the answer in the context, respond with \"The answer is not available in the context.\" Do not make up or infer any information that is not explicitly stated in the context.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Instructions:\n",
    "    1. Only use information from the provided context\n",
    "    2. If the exact answer is in the context, provide it\n",
    "    3. If the answer is not in the context, say \"The answer is not available in the context\"\n",
    "    4. Keep the answer concise and relevant\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    llm = Ollama(model=\"llama3.1\", temperature=0.3)  # Load the Llama model\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",  # Use similarity search to retrieve relevant chunks\n",
    "        search_kwargs={\n",
    "            \"k\": 5,                   # Number of relevant chunks to retrieve\n",
    "            \"fetch_k\": 15             # Total chunks to fetch\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a QA chain that combines LLM with the retriever\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # Use the \"stuff\" type for this chain\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": PROMPT,\n",
    "        },\n",
    "        return_source_documents=True  # Optionally return source documents for transparency\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n",
    "\n",
    "# Example usage\n",
    "qa_chain = create_qa_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.conversations = []\n",
    "\n",
    "    def add_conversation(self, user_input, bot_response):\n",
    "        self.conversations.append({'user': user_input, 'bot': bot_response})\n",
    "\n",
    "    def get_context(self):\n",
    "        return \"\\n\".join([f\"User: {conv['user']}\\nBot: {conv['bot']}\" for conv in self.conversations])\n",
    "\n",
    "# Example usage in handle_user_input\n",
    "memory = Memory()\n",
    "\n",
    "def handle_user_input(user_question):\n",
    "    context = memory.get_context()\n",
    "    try:\n",
    "        qa_chain = create_qa_chain()  # Create the QA chain\n",
    "        response = qa_chain.invoke({\n",
    "            \"query\": f\"{context}\\nUser: {user_question}\"  # Include memory context\n",
    "        })\n",
    "        \n",
    "        answer = response.get('result', '').strip()  # Extract the answer\n",
    "        memory.add_conversation(user_question, answer)  # Store the conversation\n",
    "\n",
    "        st.write(\"Reply: \", answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {str(e)}\")\n",
    "        st.write(\"Reply: The answer is not available in the context.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
